{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Get Modules"
      ],
      "metadata": {
        "id": "XtmVYROLVBTp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Modules"
      ],
      "metadata": {
        "id": "krZReBQcRGH2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nk0CbC6SckBj",
        "outputId": "501dd989-b4d2-425a-80a8-8bec003e7293"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.3-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m110.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m86.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.6.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.16.3 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.13.1-py3-none-any.whl (486 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.2/486.2 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting dill<0.3.7,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.65.0)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.16.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.6.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.5.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.13.1 dill-0.3.6 multiprocess-0.70.14 xxhash-3.2.0\n",
            "Collecting arabert\n",
            "  Downloading arabert-1.0.1-py3-none-any.whl (179 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PyArabic (from arabert)\n",
            "  Downloading PyArabic-0.6.15-py3-none-any.whl (126 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.4/126.4 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting farasapy (from arabert)\n",
            "  Downloading farasapy-0.0.14-py3-none-any.whl (11 kB)\n",
            "Collecting emoji==1.4.2 (from arabert)\n",
            "  Downloading emoji-1.4.2.tar.gz (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m185.0/185.0 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from farasapy->arabert) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from farasapy->arabert) (4.65.0)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from PyArabic->arabert) (1.16.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->farasapy->arabert) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->farasapy->arabert) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->farasapy->arabert) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->farasapy->arabert) (3.4)\n",
            "Building wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.4.2-py3-none-any.whl size=186458 sha256=6baf58647f7564b2e48a4fb831f5fa806970c7e5092153bae0a7b33edfff9d14\n",
            "  Stored in directory: /root/.cache/pip/wheels/10/f0/fd/4813b1177405693e8da9cdea839f0fb64fde161380e058c827\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji, PyArabic, farasapy, arabert\n",
            "Successfully installed PyArabic-0.6.15 arabert-1.0.1 emoji-1.4.2 farasapy-0.0.14\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n",
            "Collecting arabicnlp\n",
            "  Downloading arabicnlp-0.1.7-py3-none-any.whl (14.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.4/14.4 MB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from arabicnlp) (2.12.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (from arabicnlp) (2.12.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->arabicnlp) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->arabicnlp) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->arabicnlp) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->arabicnlp) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->arabicnlp) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->arabicnlp) (1.56.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->arabicnlp) (3.8.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow->arabicnlp) (0.4.10)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->arabicnlp) (16.0.0)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow->arabicnlp) (1.22.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow->arabicnlp) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow->arabicnlp) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->arabicnlp) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow->arabicnlp) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->arabicnlp) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow->arabicnlp) (2.12.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->arabicnlp) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->arabicnlp) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow->arabicnlp) (4.6.3)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->arabicnlp) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->arabicnlp) (0.32.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow->arabicnlp) (0.40.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow->arabicnlp) (0.2.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow->arabicnlp) (1.10.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->arabicnlp) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->arabicnlp) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->arabicnlp) (3.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->arabicnlp) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->arabicnlp) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->arabicnlp) (2.3.6)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->arabicnlp) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->arabicnlp) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->arabicnlp) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow->arabicnlp) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->arabicnlp) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->arabicnlp) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->arabicnlp) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->arabicnlp) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow->arabicnlp) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->arabicnlp) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow->arabicnlp) (3.2.2)\n",
            "Installing collected packages: arabicnlp\n",
            "Successfully installed arabicnlp-0.1.7\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install arabert\n",
        "!pip install sentencepiece\n",
        "!pip install  arabicnlp"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import the used Modules"
      ],
      "metadata": {
        "id": "wVYwv5L8RD9H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9lRRnysD9aCC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "gQFLX2iqRZhI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ID_ZWMWcrXL"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "def _remove_diacritics(text):\n",
        "  \"\"\"\n",
        "  this method for remove diacritics\n",
        "\n",
        "  Parameters:\n",
        "    text: string text as input\n",
        "\n",
        "  return:\n",
        "    text without diacritics\n",
        "  \"\"\"\n",
        "    text = re.sub(r'\\s*[A-Za-z]+\\b', '' , text)\n",
        "    return  re.sub(r\"[ًًٌٍَُِّْ]\", \"\", text)\n",
        "\n",
        "def _remove_extra_spaces(text):\n",
        "    \"\"\"\n",
        "    this method for remove extra spaces\n",
        "\n",
        "    Parameters:\n",
        "      text: string text as input\n",
        "\n",
        "    return:\n",
        "      text without extra spaces\n",
        "    \"\"\"\n",
        "    return re.sub(\" +\", \" \", text)\n",
        "\n",
        "def _add_spaces_to_all_special_chars(text):\n",
        "    \"\"\"\n",
        "    this method for add white space after special chars like\n",
        "\n",
        "    Parameters:\n",
        "      text: string text as input\n",
        "\n",
        "    return:\n",
        "      text without diacritics\n",
        "    \"\"\"\n",
        "    return re.sub(r\"(?<=\\w)([؟.,،])\", r\" \\1\", text)\n",
        "\n",
        "def _remove_repeated_chars(text):\n",
        "    \"\"\"\n",
        "    this method for remove repeatead chars in the word\n",
        "\n",
        "    Parameters:\n",
        "      text: string text as input\n",
        "\n",
        "    return:\n",
        "      text withoout repeatead chars\n",
        "    \"\"\"\n",
        "    return re.sub(r\"(.)\\1+\", r\"\\1\\1\", text)\n",
        "\n",
        "def _remove_qoutes(text):\n",
        "    \"\"\"\n",
        "    this method for remove quotes\n",
        "\n",
        "    Parameters:\n",
        "      text: string text as input\n",
        "\n",
        "    return:\n",
        "      text without quotes\n",
        "    \"\"\"\n",
        "    #text = re.sub(r'\"(.*?)\"',\"\",text) # remove the text between two double qoutes\n",
        "    #text = re.sub(\"https?:\\/\\/.*[\\r\\n]*\", \"\", text) # remove the urls\n",
        "    text = re.sub(r\"[0-9]\",\" \",text)\n",
        "    return re.sub(r\"\\[(.*?)\\]\", \" \" , text) # remove the text between two brackets\n",
        "\n",
        "def _remove_puncs(text):\n",
        "    \"\"\"\n",
        "    this method for preprocessing puncs\n",
        "\n",
        "    Parameters:\n",
        "      text: string text as input\n",
        "\n",
        "    return:\n",
        "      text after preprocessing puncs\n",
        "    \"\"\"\n",
        "    puncs = string.punctuation.replace(\",\",\"\").replace(\"،\",\"\").replace(\"?\",\"؟\")\n",
        "    puncs = puncs.replace(\")\",\"\").replace(\"(\",\"\")\n",
        "    text = text.translate(str.maketrans(' ', ' ', puncs))\n",
        "    text = text.replace(\"•\",\"\")\n",
        "    text = text.replace(\")\",\" \").replace(\"(\",\" \").replace(\":\",\"\")\n",
        "    return text\n",
        "\n",
        "def Sequential(fns):\n",
        "    \"\"\"\n",
        "    this method for sequential functions\n",
        "\n",
        "    Parameters:\n",
        "      fns: list of functions\n",
        "\n",
        "    return:\n",
        "      new function in new sequential function\n",
        "    \"\"\"\n",
        "    def new_fn(inputs):\n",
        "      for fn in fns:\n",
        "        inputs = fn(inputs)\n",
        "      return inputs\n",
        "    return new_fn\n",
        "\n",
        "# some of stop words we will remove from dataset\n",
        "\n",
        "WS = ['نشاط إثرائي اقرأ ثم استنتج','عرف ذلك من النص التالي','السخلة : هي ولد الفتم من الضأن والماعز ساعة وضعه',\n",
        "      'مصر من الفتح الإسلامي حتى قيام الدول المستقلة',\n",
        "      'مصر في عصر الولاة','اقرأ','لتتعرف','التالي','ذلك','النص',\n",
        "      'لاحظ الخريطة التالية لتتعرفها','فكر وناقش']\n",
        "\n",
        "WS2 = ['مرحلة التاسيس و الاستقرار',\n",
        "       'كل من',\n",
        "       'اقرأ الشكل التالي لتتعرف أشهرهم بشيء من التفصيل',\n",
        "       'بشيء من التفصيل',\n",
        "       'حمورابي بالأكدية تلفظ امورابي وتعني المعتلي',\n",
        "       'والآن اقرأ الشكل التالي لتتعرف أدوار الجهاد ضد الصليبيين',\n",
        "       '(بدر الجمالي)','هل تعرف لمـاذا','لاحظ الخريطة الزمنية التالية لتتعرفها',\n",
        "       'ومن خلال','الخريطة الزمنية السابقة'\n",
        "       ,'نستنتج أن',\n",
        "       'رضي الله عنه',\n",
        "       'نعم يا أحبابنا','ق . م',\n",
        "       'التواصل الثقافي والفني مع أفريقيا',\n",
        "       'الصـور المقابلة',\n",
        "       'مـن خـلال',\n",
        "       'الدرس الثاني',\n",
        "       'الدرس الثالث',\n",
        "       'الدرس الرابع',\n",
        "       'الدرس الخامس',\n",
        "       'الدرس',\n",
        "       'ولنبدأ',\n",
        "       'والان تعال',\n",
        "       'والآن تعال معا',\n",
        "       'استثمر المصرى',\n",
        "       'من خلال ما يلي','والآن تعال معنا',\"أولا\",\"رابعا\",\"خامسا\",\"سادسا\",\"سابعا\",\"والآن لاحظ الشكل التالي\"]\n",
        "\n",
        "WORDS=['هل','اقرأ وناقش','معلومة إثرائية','علام يدل','للصف الثاني الثانوي','أ-','ب-','ج-','وهذا ما سوف نتعرفه في الدرس القادم','عزيزي الطالب / عزيزتي الطالبة',\n",
        "         'اقرأ النص التالي','ولعلك تتساءل عزيزي الطالب','يمكنك الإجابة عن هذا التساؤل بعد قراءة النص التالي','وذلك ما سوف نتعرفه في الدرس القادم',\n",
        "  'هل تعرف أسباب فشلها  اقرأ الشكل التالي لتتعرف أهمها','تعرف ذلك من النص التالي',\"لاحظ الخريطة التالية\",\n",
        "          \"وذلك ما سوف نتعرفه في الدرس القادم\",\"والآن تعال معا\",\n",
        "       'الفتوحات الأموية في الغرب الإسلامي',\"ثالثا\",\"ثانيا\",\n",
        "       \"اولا\",\"اقرأ وناقش،المعاني والقيم الواردة\",\n",
        "       \"الفتوحات الإسلامية في عصر الدولة الأموية\",\n",
        "       'لاحظ الخريطة التالية لتتعرف عليها',\n",
        "       \"بعد ان\",\"وبعد ان القينا الضوء علي\",\n",
        "       'وهذا ما سوف نتعرفه',\n",
        "       'الوحدة التالية',\n",
        "       'رواه مسلم','العامل التاريخي',\n",
        "       'لتتعرفها','النص التالي',\"أ.\",\"ب.\",\"ج.\",\"د.\",\"والآن لاحـظ الشك\",\"ثانيا:\",\".. الرسم والنقش:\",\"سادسا:\"]\n",
        "\n",
        "\n",
        "WORDS += WS\n",
        "WORDS += WS2\n",
        "WORDS = list(set(WORDS)) # set of all words we collected\n",
        "# generate the sequential function one after another\n",
        "text_processor = Sequential([ _remove_diacritics,\n",
        "                             _remove_qoutes,\n",
        "                              _remove_puncs,\n",
        "                              _remove_extra_spaces,\n",
        "                             _remove_repeated_chars ])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modeling"
      ],
      "metadata": {
        "id": "dE_K9ZjsVuSM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pretrained model"
      ],
      "metadata": {
        "id": "HnjOdLvFVyE-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def freeze(model):\n",
        "  \"\"\"\n",
        "    this method to freeze the model\n",
        "    Parameters:\n",
        "      fns: model\n",
        "    return:\n",
        "      new freezed model\n",
        "    \"\"\"\n",
        "    for p in model.parameters():\n",
        "        p.requires_grad = False\n",
        "    return model"
      ],
      "metadata": {
        "id": "jDqiMtavCsOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGIQ-b2QOlF6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8884b03-c533-46b9-aeac-a3860d9aa7f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num of parameters is :139221504\n"
          ]
        }
      ],
      "source": [
        "# Load The Pretrained Model\n",
        "from transformers import BertTokenizer,AutoModelForSeq2SeqLM, pipeline,AutoModel,AutoTokenizer,BartForConditionalGeneration\n",
        "\n",
        "\n",
        "model_mini= \"asafaya/bert-mini-arabic\"\n",
        "model_med = \"asafaya/bert-medium-arabic\"\n",
        "model_bart = \"moussaKam/AraBART\"\n",
        "model_bart_2 = \"abdalrahmanshahrour/auto-arabic-summarization\"\n",
        "model_name=\"malmarjeh/mbert2mbert-arabic-text-summarization\"\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_bart_2)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_bart_2)\n",
        "\n",
        "#Number of  model parameters\n",
        "print(f\"num of parameters is :{sum([p.numel() for p in model.parameters()])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3bupYx-OVos"
      },
      "outputs": [],
      "source": [
        "# Load the Labeled Dataset\n",
        "import pandas as pd\n",
        "vals = pd.read_json('/content/drive/MyDrive/data/labeled_validation_dataset.jsonl',lines=True)\n",
        "vals['document'] = vals['paragraph']\n",
        "del vals['paragraph']\n",
        "vals.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vclEQiLaiRYx",
        "outputId": "76ee1873-5f9a-434e-f990-f41d341ae615"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "778"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "get_len = lambda x: len(tokenizer.tokenize(x)) # function to get the tokenization length\n",
        "test_data = pd.read_json('/content/drive/MyDrive/data/validation_data.jsonl',lines=True)\n",
        "test_data['document'] = test_data['paragraph']\n",
        "del test_data['paragraph']\n",
        "test_data['document_len'] = test_data['document'].apply(get_len)\n",
        "test_data['document_len'].max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYqorADSEjkm",
        "outputId": "00f620c0-69f1-4ddf-f589-d60af722b06c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "274"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "vals['document_len'] = vals['document'].apply(get_len)\n",
        "vals['summary_len'] = vals['summary'].apply(get_len)\n",
        "vals['document_len'].min()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data['summary'] = pred['summary']\n",
        "test_data['summary_len'] = test_data['summary'].apply(get_len)\n",
        "test_data.head()"
      ],
      "metadata": {
        "id": "ALckL3zlGea3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the Filtered Dataset\n",
        "data = pd.read_csv('/content/drive/MyDrive/data/first_9.csv')\n",
        "data2 = pd.read_csv(\"/content/drive/MyDrive/data/all_data_25_45_words.csv\")\n",
        "del data['address']\n",
        "del data2['Unnamed: 0']\n",
        "data = pd.concat([data,data2]).drop_duplicates().reset_index(drop=True)\n",
        "data['document'] = data['article']\n",
        "del data['article']\n",
        "len(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bd_eBW9jZM--",
        "outputId": "be74e2ec-e17e-4c99-a646-002dff90573f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "209933"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the summary length and document length into new columns\n",
        "data['sum_len'] = data['summary'].apply(lambda x:len(x.split()))\n",
        "data['para_len']= data['document'].apply(lambda x:len(x.split()))"
      ],
      "metadata": {
        "id": "e9s6Y23vxot4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The length Ratio of summary length and document length\n",
        "data['calc'] = data['sum_len']/data['para_len']\n",
        "data = data[(data['calc']<0.40)& (data['calc']>0.3)].reset_index(drop=True).drop_duplicates()\n",
        "len(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AsCa36lT6PTH",
        "outputId": "bc3bc4e5-8932-4ee7-d3ae-e4788ebde786"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "63604"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The length Ratio of summary length and document length for the validation\n",
        "\n",
        "vals['sum_len'] = vals['summary'].apply(lambda x:len(x.split()))\n",
        "vals['para_len']= vals['document'].apply(lambda x:len(x.split()))\n",
        "test_data['sum_len'] = test_data['summary'].apply(lambda x:len(x.split()))\n",
        "test_data['para_len']= test_data['document'].apply(lambda x:len(x.split()))\n",
        "\n",
        "vals['calc'] = vals['sum_len']/vals['para_len']\n",
        "test_data['calc'] = test_data['sum_len']/test_data['para_len']"
      ],
      "metadata": {
        "id": "lBM9ariz08QH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combining The  dataset into one big Dataset\n",
        "test_data = test_data[['document','summary']]\n",
        "vals = vals[['document','summary']]\n",
        "data = data[['document','summary']]\n",
        "data = pd.concat([data,vals]).reset_index(drop=True).drop_duplicates()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgjtpSTc1hPR",
        "outputId": "8c13bd83-0f40-4d4f-e2dc-b0a1b6c27ca9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "63717"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2LCuC5TElaSY"
      },
      "outputs": [],
      "source": [
        "\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "\n",
        "# crearting the Summarization Dataset for DataLoading\n",
        "class SummarizationDataset(Dataset):\n",
        "  \"\"\"\n",
        "  Constructor\n",
        "\n",
        "     data : Dataframe to get the data from\n",
        "  \"\"\"\n",
        "  def __init__(self,\n",
        "               data,\n",
        "               with_summary=True):\n",
        "      self.data = data\n",
        "      self.with_summary = with_summary\n",
        "  \"\"\"\n",
        "  Dender method to get the data length\n",
        "  \"\"\"\n",
        "  def __len__(self):\n",
        "      return len(self.data)\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    \"\"\"\n",
        "    Dender method to iterate over the dataframe\n",
        "    Parameters:\n",
        "      idx: int index for data\n",
        "    return:\n",
        "      the data point (document, summary)\n",
        "    \"\"\"\n",
        "     document = self.data.iloc[idx]['document']\n",
        "     document = text_processor(document)\n",
        "     document = _remove_extra_spaces(document.replace(\"\\n\",\" \"))\n",
        "     if self.with_summary:\n",
        "        summary = self.data.iloc[idx]['summary']\n",
        "        return document,summary\n",
        "     return document\n",
        "\n",
        "# To Create the Batches\n",
        "def collate_fn(batch):\n",
        "    \"\"\"\n",
        "    This method  Create the Batches\n",
        "    Parameters:\n",
        "      batch : list of data points\n",
        "\n",
        "    return:\n",
        "      transformed batch\n",
        "    \"\"\"\n",
        "    inputs,labels = [],[]\n",
        "    with_summary =  len(batch[0])>1\n",
        "    for item in batch:\n",
        "       inputs.append(item[0])\n",
        "       if with_summary:\n",
        "        labels.append(item[1])\n",
        "\n",
        "    inputs = tokenizer(inputs,\n",
        "                       return_tensors='pt',\n",
        "                       padding='longest',\n",
        "                       truncation=True,\n",
        "                       max_length=500)\n",
        "    if with_summary:\n",
        "      labels =  tokenizer(labels,\n",
        "                       return_tensors='pt',\n",
        "                       padding='longest',\n",
        "                       truncation=True,\n",
        "                       max_length=250)\n",
        "      dec_in = {k:v[:,:-1] for k,v in labels.items()}\n",
        "      dec_out = {k:v[:, 1:] for k,v in labels.items()}\n",
        "      dec_in = {'decoder_input_ids':dec_in['input_ids'],\n",
        "                \"decoder_attention_mask\":dec_in['attention_mask']}\n",
        "      inputs.update(dec_in)\n",
        "      return inputs,dec_out\n",
        "    return inputs\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate training and validation the dataset instance\n",
        "train_ds = SummarizationDataset(data)\n",
        "val_ds = SummarizationDataset(test_data)\n",
        "\n",
        "\n",
        "# Generate DataLoader or Data iterator\n",
        "train_iter = DataLoader(train_ds,batch_size=18,\n",
        "                        shuffle=True,\n",
        "                        num_workers=2,\n",
        "                        pin_memory=True,\n",
        "                        collate_fn=collate_fn)\n",
        "\n",
        "# Generate DataLoader or Data iterator\n",
        "val_iter = DataLoader(val_ds,\n",
        "                      batch_size=4,\n",
        "                      num_workers=2,\n",
        "                      pin_memory=True,\n",
        "                      collate_fn=collate_fn)\n"
      ],
      "metadata": {
        "id": "BSS0o3240zCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MBaZcI4f0ZZ"
      },
      "outputs": [],
      "source": [
        "# model configration\n",
        "model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "model.config.vocab_size = tokenizer.vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R55Pi9ijoxQ2"
      },
      "outputs": [],
      "source": [
        "loss_obj = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)   # loss objective\n",
        "def accuracy(preds,labels,attn_mask):\n",
        "  \"\"\"\n",
        "  accuracy function that simulate rouge-1 metric\n",
        "\n",
        "  Parameters:\n",
        "    preds: prediction\n",
        "    labels:  Ground Truth\n",
        "    attn_mask: attention mask\n",
        "\n",
        "  return:\n",
        "    accuracy\n",
        "  \"\"\"\n",
        "    dim = preds.shape[-1]\n",
        "    pred = preds.view(-1,dim)\n",
        "    label = labels.reshape(-1)\n",
        "    attn= attn_mask.reshape(-1)\n",
        "    pred = pred.argmax(dim=-1)\n",
        "    acc = (pred==label)*attn\n",
        "    return acc.sum()/attn.sum()\n",
        "\n",
        "def loss_fn(logits,targets):\n",
        "    \"\"\"\n",
        "    loss function\n",
        "\n",
        "    Parameters:\n",
        "      logits: prediction\n",
        "      targets:  Ground Truth\n",
        "\n",
        "    return:\n",
        "      loss\n",
        "    \"\"\"\n",
        "    return loss_obj(logits.reshape(-1,logits.size(-1))\n",
        "                     ,targets.reshape(-1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCAtgzdOoxau"
      },
      "outputs": [],
      "source": [
        "from torch.cuda.amp import GradScaler # for mixed precision\n",
        "\n",
        "\n",
        "mean = lambda x:sum(x)/len(x)\n",
        "DEVICE = torch.device(\"cuda\")\n",
        "scaler = GradScaler()\n",
        "\n",
        "\n",
        "def train_epoch(model,train_iter,opt,times=5):\n",
        "    \"\"\"\n",
        "    Training function for one loop across the training dataset\n",
        "\n",
        "    Parameters:\n",
        "      model : PretrainedModel\n",
        "      train_iter: train_iterator\n",
        "      opt: Optimizer\n",
        "      times: number of logs printing througout the training process\n",
        "\n",
        "    return:\n",
        "      mean_loss, mean_acc\n",
        "    \"\"\"\n",
        "    losses,accs = [],[]\n",
        "    model.train()\n",
        "    for i,(enc_in,labels) in enumerate(train_iter):\n",
        "      enc_in = {k:v.to(DEVICE) for k,v in enc_in.items()}\n",
        "      labels = {k:v.to(DEVICE) for k,v in labels.items()}\n",
        "      opt.zero_grad()\n",
        "      with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
        "        out = model(**enc_in).logits\n",
        "        loss = loss_fn(out,labels['input_ids'])\n",
        "      scaler.scale(loss).backward()\n",
        "      scaler.step(opt)\n",
        "      scaler.update()\n",
        "      loss = loss.item()\n",
        "      acc = accuracy(out,\n",
        "                     labels['input_ids'],\n",
        "                     labels['attention_mask']).item()\n",
        "      del out, enc_in, labels\n",
        "      torch.cuda.empty_cache()\n",
        "      if (i+1)%(len(train_iter)//times)==0:\n",
        "        print(f\"Finished Training on {(i+1)*100/len(train_iter):.2f} % of the data, loss:{loss:.3f}, acc:{acc:.3f}.\")\n",
        "      losses.append(loss)\n",
        "      accs.append(acc)\n",
        "    return mean(losses),mean(accs)\n",
        "\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def val_epoch(model,val_iter):\n",
        "   \"\"\"\n",
        "    Validation function for one loop across the validation dataset\n",
        "\n",
        "    Parameters:\n",
        "      model : PretrainedModel\n",
        "      val_iter: train_iterator\n",
        "    return:\n",
        "      mean_loss, mean_acc\n",
        "    \"\"\"\n",
        "    losses,accs = [],[]\n",
        "    model.eval()\n",
        "    for enc_in,labels in val_iter:\n",
        "        enc_in = {k:v.to(DEVICE) for k,v in enc_in.items()}\n",
        "        labels = {k:v.to(DEVICE) for k,v in labels.items()}\n",
        "        with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
        "             out = model(**enc_in).logits\n",
        "             loss = loss_fn(out,labels['input_ids'])\n",
        "        loss = loss.item()\n",
        "        acc = accuracy(out,\n",
        "                      labels['input_ids'],\n",
        "                      labels['attention_mask']).item()\n",
        "        losses.append(loss)\n",
        "        accs.append(acc)\n",
        "        del out,enc_in,labels\n",
        "        torch.cuda.empty_cache()\n",
        "    return mean(losses),mean(accs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qrlp26kWZGcw"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "class LoraLayer(nn.Module):\n",
        "  \"\"\"\n",
        "  Constructor\n",
        "   Parameters:\n",
        "    orig_layer: The layer to apply Lora on\n",
        "    rank: A and B matrix rank\n",
        "    drop: dropout rate\n",
        "    alpha: scaling factor\n",
        "\n",
        "  \"\"\"\n",
        "  def __init__(self,\n",
        "               orig_layer,\n",
        "               rank=32,\n",
        "               drop=0.1,\n",
        "               alpha=32.0):\n",
        "      super().__init__()\n",
        "      assert rank > 0\n",
        "      self.scale = alpha/rank\n",
        "      weight_shape = orig_layer.weight.shape\n",
        "\n",
        "      self.drop = nn.Dropout(drop) if drop !=0 else nn.Idenity()\n",
        "      self.lora_A = nn.Parameter(torch.zeros(rank,weight_shape[1])) # (rank, in_shape)\n",
        "      self.lora_B = nn.Parameter(torch.zeros(weight_shape[0],rank)) # (out_shape, rank)\n",
        "      nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
        "      nn.init.zeros_(self.lora_B)\n",
        "      self.orig_layer = orig_layer\n",
        "      self.merged = False\n",
        "  \"\"\"\n",
        "  This method to reset the parameters\n",
        "  \"\"\"\n",
        "  def reset_parameters(self):\n",
        "      self.orig_layer.reset_parameters()\n",
        "      nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
        "      nn.init.zeros_(self.lora_B)\n",
        "  \"\"\"\n",
        "  This method merge the parameters\n",
        "  \"\"\"\n",
        "  def merge(self):\n",
        "      self.orig_layer.weight.data += (self.lora_B @ self.lora_A).transpose(0,1)*self.scale\n",
        "      self.merged=True\n",
        "\n",
        "  def forward(self,x):\n",
        "      \"\"\"\n",
        "        This method forward pass\n",
        "          Parameters: x the inputs\n",
        "        return output\n",
        "      \"\"\"\n",
        "      out =self.orig_layer(x)\n",
        "      if not self.merged:\n",
        "         out2 = self.drop(x) @ self.lora_A.transpose(0,1)\n",
        "         out2 = (out2  @ self.lora_B.transpose(0,1))*self.scale\n",
        "         return out2 + out\n",
        "      return out\n",
        "\n",
        "# freeze the whole model except for lora parameters\n",
        "def freeze_lora(model):\n",
        "  \"\"\"Freeze the lora layer \"\"\"\n",
        "    for name,parameter in model.named_parameters():\n",
        "        if \"lora_\" not in name:\n",
        "            parameter.requires_grad=False\n",
        "    return model\n",
        "def merge_lora(ch):\n",
        "    \"\"\"\n",
        "    Merge the lora layer\n",
        "    \"\"\"\n",
        "    if not ch.merged:\n",
        "       ch.merge()  # merge the weights if its not merged\n",
        "    return ch.orig_layer\n",
        "\n",
        "\n",
        "def get_apply(Module,\n",
        "              instance,\n",
        "              attr_names,\n",
        "              map_fn,**map_fn_kwargs):\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    The this to apply specific function to specified layers in the model architecture\n",
        "\n",
        "      Parameter:\n",
        "       Module: model\n",
        "       instance: layer instance\n",
        "       attr_names:  attrs to change\n",
        "       map_fn: The applying function\n",
        "       map_fn_kwargs: the mapping function kwargs\n",
        "\n",
        "     return Model transformed\n",
        "    \"\"\"\n",
        "    for name, ch in Module.named_children():\n",
        "      if isinstance(ch,LoraLayer):\n",
        "           continue\n",
        "      elif isinstance(ch,instance):\n",
        "        for attr_name in attr_names:\n",
        "          attr = map_fn(getattr(ch,attr_name),**map_fn_kwargs)\n",
        "          setattr(ch,attr_name,attr)\n",
        "      else:\n",
        "           get_apply(ch,instance,attr_names,map_fn,**map_fn_kwargs)\n",
        "\n",
        "def convert_lora(module,**kwargs):\n",
        "  \"\"\" This function to convert specific layer to lora layer\"\"\"\n",
        "    return LoraLayer(module,**kwargs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "952r4ziHBfS_",
        "outputId": "63a44a72-c39a-4d55-cb36-5f3e926e9bc7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'trainable': 139221504, 'untrainable': 0, 'all': 139221504}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "#Get the trainable and untrainable parameters\n",
        "def get_params(model):\n",
        "    trainable = sum([p.numel() for p in model.parameters() if p.requires_grad])\n",
        "    untrainable = sum([p.numel() for p in model.parameters() if not p.requires_grad])\n",
        "    return {\"trainable\":trainable,\"untrainable\":untrainable,\"all\":(trainable+untrainable)}\n",
        "get_params(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GbTNXfJFvnV8"
      },
      "outputs": [],
      "source": [
        "\n",
        "model = model.cuda()\n",
        "opt = torch.optim.AdamW(model.parameters(),lr=5e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjLCUK55-DVy",
        "outputId": "3a27ab28-5471-47db-bd85-e1085a348f0d"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Started Training on epoch:1/2\n",
            "Finished Training on 20.00 % of the data, loss:2.850, acc:0.442.\n",
            "Finished Training on 40.00 % of the data, loss:2.923, acc:0.451.\n",
            "Finished Training on 60.00 % of the data, loss:3.041, acc:0.432.\n",
            "Finished Training on 80.00 % of the data, loss:3.221, acc:0.405.\n",
            "Finished Training on 100.00 % of the data, loss:3.039, acc:0.423.\n",
            "Finished Trainin in 30.92 mins, train_loss:3.033, train_acc:0.430,val_loss:1.456,val_acc:0.767\n",
            ".\n",
            "Started Training on epoch:2/2\n",
            "Finished Training on 20.00 % of the data, loss:2.912, acc:0.444.\n",
            "Finished Training on 40.00 % of the data, loss:2.545, acc:0.506.\n"
          ]
        }
      ],
      "source": [
        "# Training and Validation Loop\n",
        "import time\n",
        "EPOCHS = 2\n",
        "best_val = .355\n",
        "\n",
        "for e in range(EPOCHS):\n",
        "  print(f\"Started Training on epoch:{e+1}/{EPOCHS}\")\n",
        "  st = time.time()\n",
        "  train_loss, train_acc = train_epoch(model,train_iter,opt)\n",
        "  val_loss, val_acc = val_epoch(model,val_iter)\n",
        "  if val_acc>best_val:\n",
        "    best_val = val_acc\n",
        "    model.save_pretrained(f'model_{val_acc:.2f}')\n",
        "  print(f\"Finished Trainin in {(time.time()-st)/60:.2f} mins, train_loss:{train_loss:.3f}, train_acc:{train_acc:.3f},val_loss:{val_loss:.3f},val_acc:{val_acc:.3f}\\n.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAo7h9D5SKRT",
        "outputId": "c1a73d4f-21ce-45d1-909d-43830d7de279"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge) (1.16.0)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.1\n"
          ]
        }
      ],
      "source": [
        "# install the rouge metrics\n",
        "!pip install rouge\n",
        "from rouge import Rouge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRgQGexrSQy1"
      },
      "outputs": [],
      "source": [
        "rouge = Rouge()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SemSim:\n",
        "    def __init__(self,encoder_model,tokenizer):\n",
        "        for p in encoder_model.parameters():\n",
        "            p.requires_grad = False\n",
        "        self.tok = tokenizer\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.encoder = encoder.to(self.device)\n",
        "\n",
        "    def get_hiddens(self,\n",
        "                    text: str):\n",
        "        \"\"\"\n",
        "        text: str\n",
        "        return hidden states that encodes the meaning\n",
        "        \"\"\"\n",
        "        text = {k:v.to(self.device) for k,v  in self.tok(text,return_tensors='pt').items()}\n",
        "        hidden = self.encoder(**text).last_hidden_state\n",
        "        hidden = hidden.mean(dim=1)\n",
        "        return hidden\n",
        "\n",
        "    def __call__(self,\n",
        "                text1: str,\n",
        "                text2: str=None,\n",
        "                hid2=None):\n",
        "        \"\"\"\n",
        "        text1: str the first man\n",
        "        \"\"\"\n",
        "        hid1 = self.get_hiddens(text1)\n",
        "        hid2 =  self.get_hiddens(text2) if text2 is not None else hid2\n",
        "        return torch.nn.functional.cosine_similarity(hid1,hid2).item()\n",
        "\n",
        "def search(text,sent_length=12):\n",
        "    \"\"\"\n",
        "    split the long document into sentences\n",
        "    \"\"\"\n",
        "    texts = text.replace(\".\",\"،\")\n",
        "    texts = texts.split(\"،\")\n",
        "    if len(texts)<2:\n",
        "        texts = texts[0]\n",
        "        texts = texts.split()\n",
        "        word_inds = []\n",
        "        ind,lens = 0, 0\n",
        "        for i,t in enumerate(texts):\n",
        "            if (len(t)==4 or len(t)==3 or len(t) == 5):\n",
        "                if t.startswith(\"ف\") or t.startswith(\"و\"):\n",
        "                    if t not in ['وبعد',\"والي\",\"فتح\",'فرنسا',',ولايه'] and lens >sent_length:\n",
        "                        word_inds.append(ind)\n",
        "                        lens = 1\n",
        "            ind = ind + len(t) + 1\n",
        "            lens += 1\n",
        "        word_inds= [i+j for i,j in zip(word_inds, range(len(word_inds)))]\n",
        "        word_inds = [0] + word_inds if word_inds[0] !=0 else word_inds\n",
        "        word_inds =  word_inds + [len(text)-1] if (len(text)-1)!=word_inds[-1] else word_inds\n",
        "        return [text[word_inds[i]:word_inds[i+1]] for i in range(len(word_inds)-1)]\n",
        "    return texts"
      ],
      "metadata": {
        "id": "Qs288zGijyfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DocumentProcessor:\n",
        "    def __init__(self,\n",
        "                 encoder_model,\n",
        "                 tokenizer,):\n",
        "        self.semsim = SemSim(encoder_model,tokenizer)\n",
        "        self.preprocessor = Sequential([ _remove_diacritics,\n",
        "                             _remove_qoutes,\n",
        "                              _remove_puncs,\n",
        "                              _remove_extra_spaces,\n",
        "                             _remove_repeated_chars ])\n",
        "\n",
        "    def split_int_sents(self,doc,max_sent_len=12):\n",
        "        \"\"\"\n",
        "        split the document the according to the searching algorithm\n",
        "        \"\"\"\n",
        "        return search(doc,max_sent_len)\n",
        "    def smart_prepare(self,\n",
        "                      text:str,\n",
        "                      max_sub_len: int= 128,\n",
        "                      semantic_thres:float=0.25,\n",
        "                      return_hid:bool=True,):\n",
        "\n",
        "        \"\"\"Smart select the sentences according to the semantic meaning\"\"\"\n",
        "        texts = self.split_int_sents(text)\n",
        "        length = 0\n",
        "        count = -1\n",
        "        chunks = []\n",
        "        chunk = ''\n",
        "        split_on=\"،\"\n",
        "        hid = self.semsim.get_hiddens(text)\n",
        "\n",
        "        texts = [sent for sent in texts if self.semsim(sent,hid2=hid)>=semantic_thres]\n",
        "        for sent in texts:\n",
        "            count += 1\n",
        "            com_length = len(sent.split()) +length\n",
        "\n",
        "            if com_length< max_sub_len:\n",
        "              chunk = split_on.join([chunk,sent])\n",
        "              length = com_length\n",
        "\n",
        "              if count == len(texts)-1:\n",
        "                chunks.append(chunk.strip()[1:])\n",
        "\n",
        "            else:\n",
        "              chunks.append(chunk.strip()[1:])\n",
        "              chunk = ''\n",
        "              chunk = split_on.join([chunk,sent])\n",
        "              length = len(sent.split())\n",
        "        chunks = [c for c in chunks if len(c)>5]\n",
        "        if return_hid:\n",
        "          return chunks,hid\n",
        "        return chunks\n",
        "\n",
        "\n",
        "    def preprocess_text(self,doc):\n",
        "        doc = self.preprocessor(doc)\n",
        "        return _remove_extra_spaces(doc.replace(\"\\n\",\" \"))\n",
        "\n",
        "    def __call__(self,\n",
        "                 doc,\n",
        "                 max_sub_len:int=90,\n",
        "                 semantic_thres:float=0.25,\n",
        "                 return_hid=False):\n",
        "        doc = self.preprocess_text(doc)\n",
        "        return self.smart_prepare(doc,\n",
        "                             max_sub_len,\n",
        "                             semantic_thres,\n",
        "                             return_hid)\n",
        "\n"
      ],
      "metadata": {
        "id": "YOsgi9vujNpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def generate(model,tok,text,**kwargs):\n",
        "    \"\"\"\n",
        "    Generate the text\n",
        "    \"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    inputs = tok(text,return_tensors='pt',truncation=True)\n",
        "    if kwargs is not None:\n",
        "       num_returns = kwargs.get(\"num_return_sequences\",None)\n",
        "    inputs = {k:v.to(device) for k,v in inputs.items()}\n",
        "    gen = model.generate(**inputs,**kwargs)\n",
        "    gen = gen.cpu().squeeze()\n",
        "    if num_returns is not None:\n",
        "      return tok.batch_decode(gen,skip_spectial_tokens=True)\n",
        "    return tok.decode(gen,skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "pWAZcYcblcqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"your model path\")\n",
        "tokenizer = AutoTokenizer.from_pretrained('your tokenizer path')\n",
        "encoder = copy.deepcopy(model.model.encoder)\n",
        "doc_prcocessor = DocumentProcessor(encoder,tokenizer)"
      ],
      "metadata": {
        "id": "RPDtGkmxmawL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEFTikxuqRAf",
        "outputId": "3599bce3-0305-46c3-e914-ee7b44845f20"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.05769230375739672"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "# Generate a summart\n",
        "docs = doc_prcocessor(test_data['document'][1])\n",
        "sums= []\n",
        "p\n",
        "for t in docs:\n",
        "     summary = generate(model,\n",
        "             tokenizer,\n",
        "             t,\n",
        "             max_length=300,\n",
        "             num_beams=5,\n",
        "             no_repeat_ngram_size=2,\n",
        "             repetition_penalty=2.0,\n",
        "             length_penalty=1.0,\n",
        "             top_p=0.92,\n",
        "             top_k=5000)\n",
        "\n",
        "     sums.append(summary)\n",
        "summary = (\"،\"+\" \").join([s.replace('.',\"\") for s in sums])\n",
        "summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNBnU2QKHF0q"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}